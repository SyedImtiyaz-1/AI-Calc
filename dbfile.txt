
import cvzone
import cv2
from cvzone.HandTrackingModule import HandDetector
import numpy as np
import google.generativeai as genai
from PIL import Image
import streamlit as st
import sqlite3
from streamlit_webrtc import VideoTransformerBase, webrtc_streamer

# ------------------ Database Setup ------------------ #

# Connect to SQLite database (or create it if it doesn't exist)
conn = sqlite3.connect('users.db')
c = conn.cursor()

# Create a table for users if it doesn't exist
c.execute('''
    CREATE TABLE IF NOT EXISTS users (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        name TEXT NOT NULL
    )
''')
conn.commit()

# ------------------ Streamlit Configuration ------------------ #

st.set_page_config(layout="wide")
st.title("AI Gesture Calculator")

# ------------------ Application State Management ------------------ #

# Initialize session state for user authentication
if 'authenticated' not in st.session_state:
    st.session_state.authenticated = False
    st.session_state.user_name = ''

# ------------------ Name Entry Page ------------------ #

if not st.session_state.authenticated:
    st.header("Enter Your Name")
    with st.form(key='name_form'):
        name = st.text_input("Name")
        submit_button = st.form_submit_button(label='Submit')

    if submit_button:
        if name.strip() == '':
            st.error("Please enter a valid name.")
        else:
            # Save the name to the database
            c.execute('INSERT INTO users (name) VALUES (?)', (name.strip(),))
            conn.commit()
            st.session_state.authenticated = True
            st.session_state.user_name = name.strip()
            st.success(f"Welcome, {st.session_state.user_name}!")
            st.experimental_rerun()  # This might work in newer versions

# ------------------ AI Calculator Page ------------------ #

if st.session_state.authenticated:
    st.subheader(f"Hello, {st.session_state.user_name}!")

    # Layout configuration
    col1, col2 = st.columns([3, 2])

    with col1:
        run = st.checkbox('Activate Project', value=True)
        FRAME_WINDOW = st.empty()  # Placeholder for video frames

    with col2:
        st.title("Answer")
        output_text_area = st.empty()

    # Configure Generative AI
    genai.configure(api_key="YOUR_API_KEY_HERE")  # Replace with your actual API key
    model = genai.GenerativeModel('gemini-1.5-flash')

    # Initialize the HandDetector class with the given parameters
    detector = HandDetector(staticMode=False, maxHands=1, modelComplexity=1, detectionCon=0.7, minTrackCon=0.5)

    # Initialize canvas and previous position in session state
    if 'canvas' not in st.session_state:
        st.session_state.canvas = None
    if 'prev_pos' not in st.session_state:
        st.session_state.prev_pos = None

    # Initialize the canvas
    if st.session_state.canvas is None:
        st.session_state.canvas = np.zeros((720, 1280, 3), dtype=np.uint8)

    # Define a transformer class for video processing
    class HandGestureTransformer(VideoTransformerBase):
        def __init__(self):
            self.canvas = st.session_state.canvas.copy()
            self.prev_pos = st.session_state.prev_pos

        def transform(self, frame):
            img = frame.to_ndarray(format="bgr24")
            img = cv2.flip(img, 1)  # Mirror the image

            # Detect hands
            hands, img = detector.findHands(img, draw=False, flipType=True)

            if hands:
                hand = hands[0]
                lmList = hand["lmList"]
                fingers = detector.fingersUp(hand)
                print(fingers)

                if fingers == [0, 1, 0, 0, 0]:
                    current_pos = lmList[8][0:2]
                    if self.prev_pos is None:
                        self.prev_pos = current_pos
                    cv2.line(self.canvas, tuple(current_pos), tuple(self.prev_pos), (0, 0, 255), 10)
                    self.prev_pos = current_pos
                elif fingers == [1, 0, 0, 0, 0]:
                    self.canvas = np.zeros_like(img)
                    self.prev_pos = None
                elif fingers == [1, 1, 1, 1, 0]:
                    # Trigger AI response
                    pil_image = Image.fromarray(cv2.cvtColor(self.canvas, cv2.COLOR_BGR2RGB))
                    try:
                        response = model.generate_content(["Solve this math problem", pil_image])
                        st.session_state.output_text = response.text
                    except Exception as e:
                        st.session_state.output_text = f"AI Error: {e}"

            # Update session state
            st.session_state.canvas = self.canvas.copy()
            st.session_state.prev_pos = self.prev_pos

            # Combine the webcam image with the canvas
            image_combined = cv2.addWeighted(img, 0.7, self.canvas, 0.3, 0)

            return image_combined

    # Use streamlit-webrtc to capture and process video
    webrtc_ctx = webrtc_streamer(
        key="ai-gesture-calculator",
        mode="transform",
        rtc_configuration={
            "iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]
        },
        video_transformer_factory=HandGestureTransformer,
        async_transform=True,
    )

    # Display the combined image
    if webrtc_ctx.video_transformer:
        FRAME_WINDOW.image(webrtc_ctx.video_transformer.transform(webrtc_ctx.video_transformer.last_frame), channels="BGR")

    # Display AI response
    if 'output_text' in st.session_state:
        output_text_area.text(st.session_state.output_text)

# ------------------ Cleanup ------------------ #

# Close the database connection when the app stops
def cleanup():
    conn.close()

import atexit
atexit.register(cleanup)
